{"cells":[{"cell_type":"markdown","metadata":{"id":"2U8cXJlMI8fB"},"source":["# Pip installs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54569,"status":"ok","timestamp":1649346183710,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-120},"id":"BZGbcEOhVidt","outputId":"3ce6da95-e674-467b-f830-c82ee07f013c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting efficientnet_pytorch\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.10.0.2)\n","Building wheels for collected packages: efficientnet-pytorch\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=5a3d08f8f1643e2b4adfa71693e4cb722764d11a4ab0d73c03d0fa30bce8b8af\n","  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n","Successfully built efficientnet-pytorch\n","Installing collected packages: efficientnet-pytorch\n","Successfully installed efficientnet-pytorch-0.7.1\n","Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Collecting bcolz\n","  Downloading bcolz-1.2.1.tar.gz (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 39.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from bcolz) (1.21.5)\n","Building wheels for collected packages: bcolz\n","  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bcolz: filename=bcolz-1.2.1-cp37-cp37m-linux_x86_64.whl size=2649931 sha256=07dbdfd8c40d05bc4cedf99a7c400367df5233cab9986a851c1c524de15f44ab\n","  Stored in directory: /root/.cache/pip/wheels/2c/35/ca/9d914de345914e2446ea285170329f771b8abba2a00f7650bd\n","Successfully built bcolz\n","Installing collected packages: bcolz\n","Successfully installed bcolz-1.2.1\n","Collecting einops\n","  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n","Installing collected packages: einops\n","Successfully installed einops-0.4.1\n"]}],"source":["!pip install efficientnet_pytorch\n","!pip install spacy\n","!pip install bcolz\n","!pip install einops"]},{"cell_type":"markdown","metadata":{"id":"jjjO8X6_Xnaz"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4s_TmhWWOlB"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch.backends.cudnn as cudnn\n","import numpy as np\n","import torchvision\n","import pickle\n","from torchvision.io import read_image\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import shutil\n","import os\n","import copy\n","import torchvision.transforms as T\n","from torchsummary import summary\n","from torchvision.models import efficientnet_b4,efficientnet_b0\n","import random\n","import pandas as pd\n","import json\n","from efficientnet_pytorch import EfficientNet\n","import spacy\n","from PIL import Image\n","from torch.utils.data import DataLoader, Dataset\n","from spacy.attrs import ORTH\n","import cv2 as cv \n","from google.colab.patches import cv2_imshow\n","from einops import rearrange\n","import math\n","from efficientnet_pytorch import EfficientNet\n","import bcolz"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23797,"status":"ok","timestamp":1649346215386,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-120},"id":"kVOMi8PkaLHT","outputId":"8893f972-57f4-454a-c81c-3c3afd8573b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"markdown","metadata":{"id":"lS6ulGHPuVLz"},"source":["# Flickr Dataset"]},{"cell_type":"code","source":["!unzip '/content/drive/MyDrive/GroupProjectNLP/flickr8k.zip'"],"metadata":{"id":"KeMPoyVr3_KS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmVG0pOxvHtF"},"outputs":[],"source":["!unzip '/content/drive/MyDrive/GroupProjectNLP/flickr30k.zip'"]},{"cell_type":"markdown","source":["# Preprocess the images, captions and create a data generator\n"],"metadata":{"id":"ofZ9SIBr4Eze"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GnIvDj94uXQ9"},"outputs":[],"source":["#Data loader for training: will return only one sentence per image\n","class FlickrDataset(Dataset):\n","    \n","    def __init__(self, root, ann_file, img_transform=None, txt_transform=None):\n","        df = pd.read_csv(ann_file)\n","        self.root = root\n","        self.img_transform = img_transform\n","        self.txt_transform = txt_transform\n","        self.img_ids = df['image']\n","        self.captions = df['caption']\n","        self.vocab = Vocabulary()\n","        self.vocab.build_vocab(self.captions.tolist())\n","    \n","    def __len__(self):\n","        return len(self.img_ids)\n","\n","    def __getitem__(self, idx):\n","        img = Image.open(os.path.join(self.root, self.img_ids[idx]))\n","        img = self.img_transform(img) if self.img_transform is not None else img\n","        caption = self.captions[idx]\n","        caption = self.txt_transform(caption) if self.txt_transform is not None else caption\n","        caption = self.vocab.numericalize('<sos> ' + caption.lstrip() + ' <eos>')\n","        return img, torch.Tensor(caption)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vK1N1lJBww48"},"outputs":[],"source":["#Data loader for validation/testing: will return 5 sentences per image\n","class FlickrDatasetVal(Dataset):\n","    \n","    def __init__(self, root, ann_file, img_transform=None, txt_transform=None):\n","        df = pd.read_csv(ann_file)\n","        self.root = root\n","        self.img_transform = img_transform\n","        self.txt_transform = txt_transform\n","        self.img_ids = df['image']\n","        self.captions = df['captions']\n","        self.vocab = Vocabulary()\n","        self.vocab.build_vocab(self.captions.tolist())\n","    \n","    def __len__(self):\n","        return len(self.img_ids)\n","\n","    def collate_captions(self, captions):\n","        lengths = [len(cap) for cap in captions]\n","        targets = torch.zeros(len(captions), max(lengths)).long()\n","        for i, cap in enumerate([torch.tensor(caption) for caption in captions]):\n","            end = lengths[i]\n","            targets[i, :end] = cap[:end]\n","        return targets  \n","\n","    def __getitem__(self, idx):\n","        img = Image.open(os.path.join(self.root, self.img_ids[idx]))\n","        img = self.img_transform(img) if self.img_transform is not None else img\n","        captions = self.captions[idx].split('|')\n","        captions = ['<sos> ' + caption.lstrip() + ' <eos>' for caption in captions]\n","        captions = self.txt_transform(captions) if self.txt_transform is not None else captions\n","        captions = [self.vocab.numericalize(caption) for caption in captions]        \n","        return img, self.collate_captions(captions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0hdxJwt1I4t"},"outputs":[],"source":["#Preprocessing for both training and validation data\n","def collate_fn(data):\n","    # Sort a data list by caption length (descending order).\n","    data.sort(key=lambda x: len(x[1]), reverse=True)\n","    images, captions = zip(*data)\n","\n","    # Merge images (from tuple of 3D tensor to 4D tensor).\n","    images = torch.stack(images, 0)\n","\n","    # Merge captions (from tuple of 1D tensor to 2D tensor).\n","    lengths = [len(cap) for cap in captions]\n","    targets = torch.zeros(len(captions), max(lengths)).long()\n","    for i, cap in enumerate(captions):\n","        end = lengths[i]\n","        targets[i, :end] = cap[:end]        \n","    return images, targets, lengths\n","\n","\n","def collate_fn_val(data):\n","    images, captions = zip(*data)\n","    images = torch.stack(images, 0)\n","    max_length = max([len(cap[0]) for cap in captions])\n","    targets = torch.zeros(len(captions), 5, max_length).long()\n","    for i, caps in enumerate(captions):\n","        end = caps.shape[1]\n","        targets[i, :, :end] = caps[:end]\n","    return images, targets, max_length\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77P9pCKXvso-"},"outputs":[],"source":["#Building the vocabulary for a given Flickr dataset\n","spacy_eng = spacy.load(\"en_core_web_sm\")\n","spacy_eng.tokenizer.add_special_case('<sos>', [{ORTH: \"<sos>\"}])\n","spacy_eng.tokenizer.add_special_case('<eos>', [{ORTH: \"<eos>\"}])\n","spacy_eng.tokenizer.add_special_case('<pad>', [{ORTH: \"<pad>\"}])\n","spacy_eng.tokenizer.add_special_case('<unk>', [{ORTH: \"<unk>\"}])\n","\n","class Vocabulary:\n","\n","    def __init__(self):\n","        self.itos = {0:\"<pad>\",1:\"<sos>\",2:\"<eos>\",3:\"<unk>\"}\n","        self.stoi = {v:k for k,v in self.itos.items()}\n","        \n","    def __len__(self): return len(self.itos)\n","    \n","    @staticmethod\n","    def tokenize(text):\n","        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n","    \n","    def build_vocab(self, sentence_list):\n","        idx = 4\n","        # add words from all sentences to vocab\n","        for sentence in sentence_list:\n","            for word in self.tokenize(sentence):\n","                if word not in self.stoi:\n","                    self.stoi[word] = idx\n","                    self.itos[idx] = word\n","                    idx += 1\n","    \n","    def numericalize(self,text):\n","        tokenized_text = self.tokenize(text)\n","        return [self.stoi[token] if token in self.stoi else self.stoi[\"<unk>\"] for token in tokenized_text]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9seKWfcub1U"},"outputs":[],"source":["# Create train, validation and test data loaders for Flickr30k\n","transform = transforms.Compose([\n","    transforms.Resize([224, 224]),\n","    transforms.ToTensor()\n","])\n","\n","# Training dataset & loader: will only give 1 target sentence per image\n","dataset = FlickrDataset(\n","    root='/content/flickr30k_images/flickr30k_images',\n","    ann_file='/content/drive/MyDrive/GroupProjectNLP/flickr30k_train.csv',\n","    img_transform=transform\n",")\n","\n","train_loader = DataLoader(\n","    dataset=dataset,\n","    batch_size=128,\n","    num_workers=8,\n","    shuffle=True,\n","    pin_memory=True,\n","    collate_fn=collate_fn\n",")\n","\n","# Validation: will give 5 sentences per image\n","valset = FlickrDatasetVal(\n","    root='/content/flickr30k_images/flickr30k_images',\n","    ann_file='/content/drive/MyDrive/GroupProjectNLP/flickr30k_val.csv',\n","    img_transform=transform\n",")\n","\n","val_loader = DataLoader(\n","    dataset=valset,\n","    batch_size=100,\n","    num_workers=8,\n","    shuffle=False,\n","    collate_fn=collate_fn_val\n",")\n","\n","# Testing: will give 5 sentences per image\n","testset = FlickrDatasetVal(\n","    root='/content/flickr30k_images/flickr30k_images',\n","    ann_file='/content/drive/MyDrive/GroupProjectNLP/flickr30k_test.csv',\n","    img_transform=transform\n",")\n","\n","test_loader = DataLoader(\n","    dataset=testset,\n","    batch_size=100,\n","    num_workers=8,\n","    shuffle=False,\n","    collate_fn=collate_fn_val\n",")\n","\n","#If we use GloVe embeddings with Flickr30k\n","dataset_embeddings = FlickrDataset(\n","    root='/content/flickr30k_images/flickr30k_images',\n","    ann_file='/content/drive/MyDrive/GroupProjectNLP/flickr30k_train.csv',\n","    img_transform=transform\n",")\n","\n","dataset_embeddings_val = FlickrDatasetVal(\n","    root='/content/flickr30k_images/flickr30k_images',\n","    ann_file='/content/drive/MyDrive/GroupProjectNLP/flickr30k_val.csv',\n","    img_transform=transform\n",")\n","dataset_embeddings_test = FlickrDatasetVal(\n","    root='/content/flickr30k_images/flickr30k_images',\n","    ann_file='/content/drive/MyDrive/GroupProjectNLP/flickr30k_test.csv',\n","    img_transform=transform\n",")\n","\n","\n","flickr_classes = list(dataset_embeddings.vocab.stoi.keys())\n","for word in list(dataset_embeddings_val.vocab.stoi.keys()):\n","  if word not in flickr_classes:\n","    flickr_classes.append(word)\n","for word in list(dataset_embeddings_test.vocab.stoi.keys()):\n","  if word not in flickr_classes:\n","    flickr_classes.append(word)\n","flickr_classes.append(\"<pad>\")\n"]},{"cell_type":"code","source":["# For Flickr8k\n","transform = transforms.Compose([\n","    transforms.Resize([224, 224]),\n","    transforms.ToTensor()\n","])\n","\n","# Training dataset & loader: will only give 1 target sentence per image\n","dataset = FlickrDataset(\n","    root='/content/Images',\n","    ann_file='/content/drive/MyDrive/GroupProjectNLP/flickr8k_train.csv',\n","    img_transform=transform\n",")\n","\n","train_loader = DataLoader(\n","    dataset=dataset,\n","    batch_size=128,\n","    num_workers=8,\n","    shuffle=True,\n","    pin_memory=True,\n","    collate_fn=collate_fn\n",")\n","\n","# Validation: will give 5 sentences per image\n","valset = FlickrDatasetVal(\n","    root='/content/Images',\n","    ann_file='/content/drive/MyDrive/GroupProjectNLP/flickr8k_val.csv',\n","    img_transform=transform\n",")\n","\n","val_loader = DataLoader(\n","    dataset=valset,\n","    batch_size=100,\n","    num_workers=8,\n","    shuffle=False,\n","    collate_fn=collate_fn_val\n",")\n","\n","# Testing: will give 5 sentences per image\n","testset = FlickrDatasetVal(\n","    root='/content/Images',\n","    ann_file='/content/drive/MyDrive/GroupProjectNLP/flickr8k_test.csv',\n","    img_transform=transform\n",")\n","\n","test_loader = DataLoader(\n","    dataset=testset,\n","    batch_size=100,\n","    num_workers=8,\n","    shuffle=False,\n","    collate_fn=collate_fn_val\n",")\n","\n","flickr_classes = list(dataset.vocab.stoi.keys())\n","flickr_classes.append(\"<pad>\")"],"metadata":{"id":"4ab7WheM4_uP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"duEP1CfFrQ6p"},"source":["# Feature extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Fz7-jd-rS5b"},"outputs":[],"source":["#Load feature extraction model (EfficientNet-4)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model_name = 'efficientnet-b4'\n","classifier_model = EfficientNet.from_pretrained(model_name).to(device)\n","image_size = EfficientNet.get_image_size(model_name)\n","classifier_model.eval()"]},{"cell_type":"markdown","metadata":{"id":"Kdtu9x2d53T0"},"source":["# Image preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-q2wPfjyrVQv"},"outputs":[],"source":["#Image preprocessing\n","\n","def preprocessing_with_aug(imgs):\n","  #For training\n","  gamma = np.random.choice([0.8, 1., 1.2])\n","  imgs = T.functional.adjust_gamma(imgs, gamma)\n","  transf_aug = T.Compose([T.RandomHorizontalFlip(p=0.5),\n","                          T.RandomResizedCrop((224,224)),\n","             ])\n","  imgs = transf_aug(imgs)\n","  return T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(imgs)\n","\n","def preprocessing_no_aug(imgs):\n","  #For testing, only normalize pixels\n","  return T.Compose([T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])(imgs)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LIK7hgcKb_tZ"},"source":["# Word Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yfJ5sl6kb_Xl"},"outputs":[],"source":["#Load in pickled embeddings\n","glove_path = \"/content/drive/MyDrive/GroupProjectNLP/\"\n","vectors = bcolz.open(f'{glove_path}/6B.50.dat')[:]\n","words = pickle.load(open(f'{glove_path}/6B.50_words.pkl', 'rb'))\n","word2idx = pickle.load(open(f'{glove_path}/6B.50_idx.pkl', 'rb'))\n","glove = {w: vectors[word2idx[w]] for w in words}"]},{"cell_type":"markdown","metadata":{"id":"Gl4HoNkAcCZ5"},"source":["## GloVe embedding matrix for Flickr classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUq8j-TUcfhk"},"outputs":[],"source":["# Embedding matrix of flickr classes\n","embedding_dim = 50\n","matrix_len = len(flickr_classes)\n","weights_matrix = np.zeros((matrix_len, embedding_dim))\n","words_found = 0\n","\n","#Create a matrix of embeddings, not yet directly usable by Pytorch\n","for i, word in enumerate(flickr_classes):\n","    try: \n","        weights_matrix[i] = glove[word]\n","        words_found += 1\n","    except KeyError:\n","        weights_matrix[i] = glove[\"<unk>\"]\n","\n","\n","#Transform the embedding matrix to its Pytorch equivalence\n","weights_matrix = torch.Tensor(weights_matrix)\n","non_trainable = True\n","num_embeddings, embedding_dim = weights_matrix.shape\n","emb_layer_flickr = torch.nn.Embedding(num_embeddings, embedding_dim)\n","emb_layer_flickr.load_state_dict({'weight': weights_matrix})\n","if non_trainable:\n","    emb_layer_flickr.weight.requires_grad = False\n","emb_layer_flickr"]},{"cell_type":"markdown","metadata":{"id":"VN4JK4a1LyxA"},"source":["# Function that takes batches and outputs the image features and the target caption indexes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QHOUFNOVWoVI"},"outputs":[],"source":["#For training we do data augmentation\n","def load_training(data):\n","    (imgs, targets, lengths)=data\n","\n","    with torch.no_grad():\n","      features = classifier_model.extract_features(preprocessing_with_aug(imgs).to(device)).flatten(start_dim=-2).permute(0,2,1)\n","      return(features, targets)\n","\n","\n","def load_validation(data):\n","    (imgs, targets, lengths)=data\n","\n","    with torch.no_grad():      \n","      features = classifier_model.extract_features(preprocessing_no_aug(imgs).to(device)).flatten(start_dim=-2).permute(0,2,1)\n","      return(features, targets)"]},{"cell_type":"markdown","metadata":{"id":"sSImpeB-MCVt"},"source":["# Positional encoding for the transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_OdggMj0MDrf"},"outputs":[],"source":["# Source: https://pytorch.org/tutorials/beginner/transformer_tutorial\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=100):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)"]},{"cell_type":"markdown","metadata":{"id":"Wcl2SWo-MKKi"},"source":["# Some other needed functions for the transformer (input layer, target mask, etc.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NGrBklhMXAM"},"outputs":[],"source":["class InputLayer(nn.Module):\n","  \"\"\"\n","  Reshaping the image features.\n","  Takes batch of transformed images in the form BxCxWxH and\n","\n","  img_feature_size = WxH\n","\n","  Returns BxC+Num_of_words) x Word_size\n","  \"\"\"\n","\n","  def __init__(self, img_feature_size, output_size):\n","    super().__init__()\n","\n","    self.fc = nn.Linear(img_feature_size, output_size)\n","\n","  def forward(self, batch_img_feat):\n","\n","    x = torch.flatten(batch_img_feat, 2)\n","   \n","    x = self.fc(x)\n","\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3X1qkd1MMdp7"},"outputs":[],"source":["def get_tgt_mask( size) -> torch.tensor:\n","        # Generates a squeare matrix where the each row allows one word more to be seen\n","        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n","        mask = mask.float()\n","        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n","        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n","        \n","        # EX for size=5:\n","        # [[0., -inf, -inf, -inf, -inf],\n","        #  [0.,   0., -inf, -inf, -inf],\n","        #  [0.,   0.,   0., -inf, -inf],\n","        #  [0.,   0.,   0.,   0., -inf],\n","        #  [0.,   0.,   0.,   0.,   0.]]\n","        \n","        return mask\n","    \n","def create_pad_mask( matrix: torch.tensor, pad_token: int) -> torch.tensor:\n","        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n","        # [False, False, False, True, True, True]\n","        return torch.where(matrix==pad_token, True, False)"]},{"cell_type":"markdown","metadata":{"id":"zUiW_8HaMgtQ"},"source":["# The TRANSFORMER model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QN73B5VjMjWc"},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(self, vocab_size, d_model, nhead,\n","                 dim_feedforward=2048, max_seq_length=96, pos_dropout=0.1, output_size=54, img_features_size=64, trans_dropout=0.1\n","                 , num_encoder_layers=6, num_decoder_layers=6):\n","        \"\"\"\n","        Initializes the model\n","                Parameters:\n","                        vocab_size (int): The amount of tokens in both vocabularies (including start, end, etc tokens)\n","                        d_model (int): Expected number of features in the encoder/decoder inputs, also used in embeddings\n","                        nhead (int): Number of heads in the transformer\n","                        num_encoder_layers (int): Number of sub-encoder layers in the transformer\n","                        num_decoder_layers (int): Number of sub-decoder layers in the transformer\n","                        dim_feedforward (int): Dimension of the feedforward network in the transformer\n","                        max_seq_length (int): Maximum length of each tokenized sentence\n","                        pos_dropout (float): Dropout value in the positional encoding\n","                        trans_dropout (float): Dropout value in the transformer\n","        \"\"\"\n","        super().__init__()\n","        self.d_model = d_model\n","        # self.embed_src = nn.Embedding(vocab_size, d_model)\n","        self.input = InputLayer(img_features_size, output_size)\n","\n","        # self.embed_tgt = nn.Embedding(vocab_size, d_model)\n","        #If we use GloVe embeddings in the Transformer decoder\n","        self.embed_tgt = emb_layer_flickr\n","\n","        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)\n","        #If we don't want the input dimensions to be the same as the dimensions inside the transformer\n","        # self.pos_enc = PositionalEncoding(output_size, pos_dropout, max_seq_length)\n","\n","        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)\n","        self.fc = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, src_full, tgt, \n","                tgt_mask):\n","      \n","        src = self.input(src_full)\n","        \n","        # change to seq_len x batch x 54\n","        src = torch.permute(src, (1,0,2))\n","\n","        tgt_key_padding_mask = create_pad_mask(tgt,0)\n","        tgt = torch.permute(tgt, (1,0))\n","  \n","\n","        # Embed the targets, scale by sqrt(d_model), and add the positional encoding\n","        tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))\n","        \n","\n","        # Send the batches to the model\n","        \n","        output = self.transformer(src.float(), tgt.float(), tgt_mask=tgt_mask, \n","                                  tgt_key_padding_mask=tgt_key_padding_mask, \n","                                  )\n","\n","        # change back to batch x seq_len x 54\n","        output = torch.permute(output, (1,0,2))\n","        \n","        # Run the output through an fc layer to return values for each token in the vocab\n","        return self.fc(output)"]},{"cell_type":"markdown","metadata":{"id":"-4A8qalTMt-7"},"source":["# Detokenizer and Bleu Score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":347,"status":"ok","timestamp":1649195298656,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-60},"id":"RVaV3-aGMwZI","outputId":"ba37ba08-241d-4dc6-8a0c-7c8441b57309"},"outputs":[{"data":{"text/plain":["0.8408964276313782"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["from torchtext.data.metrics import bleu_score\n","candidate_corpus = [['My' , 'full', 'pytorch' ,'test'], ['Another', 'Sentence']]\n","references_corpus = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']], [['No', 'Match']]]\n","bleu_score(candidate_corpus, references_corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-cEtxqsM0Sy"},"outputs":[],"source":["#Transforms a list of tokens into a list of strings\n","def detokenize(batch):\n","  #During training, there is only one sentence for each image in the batch\n","  (dataset.vocab.itos[4])\n","  temp = batch.tolist()\n","  op = lambda x: dataset.vocab.itos[x]\n","\n","  detokenized= []\n","  for i in range(len(temp)):\n","    withpad = list(map(op, temp[i]))\n","    woutpados = list(filter(lambda x : x not in ['<pad>', '<eos>', '<sos>', ' ', ''], withpad))\n","    detokenized.append(woutpados)\n","  \n","  return detokenized"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ejyvdbPFzus"},"outputs":[],"source":["#When we have multiple target sentences\n","def detokenize_multiple(batch, dataset):\n","  # 5 sentences per image in the batch\n","  (dataset.vocab.itos[4])\n","  batch_size, n_sentences, max_seq_length = batch.shape\n","  op = lambda x: dataset.vocab.itos[x]\n","\n","  detokenized_all= []\n","  for i in range(batch_size):\n","    detokenized_single_image = []\n","    for j in range(n_sentences):\n","      withpad = list(map(op, batch[i,j,:].tolist()))\n","      woutpados = list(filter(lambda x : x not in ['<pad>', '<eos>', '<sos>', ' ', ''], withpad))\n","      detokenized_single_image.append(woutpados)\n","    detokenized_all.append(detokenized_single_image)\n","  \n","  return detokenized_all"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4G6ERReEM3a-"},"outputs":[],"source":["#Compute the average BLEU score across a batch\n","def cal_bleu_score(predictions, targets, n=4, dataset=valset):\n","    targets = detokenize_multiple(targets, dataset)\n","    predictions = detokenize(predictions)\n","    score = round(bleu_score(predictions, targets, max_n=n, weights=[1/n]*n) * 100, 2)\n","    return score"]},{"cell_type":"markdown","metadata":{"id":"wmxqOapMM56b"},"source":["# TRAINING FUNCTION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fiwF5zNbNAxE"},"outputs":[],"source":["def train_loop(model, opt, loss_fn, train_loader):\n","    model.train()\n","    total_loss = 0.0\n","\n","    for i, batch in enumerate(train_loader,0):\n","\n","      batch_img, batch_target_seq = load_training(batch) # image, target, length of target\n","      batch_img, batch_target_seq = batch_img.clone().detach().to(device), batch_target_seq.clone().detach().to(device)\n","      X = batch_img\n","\n","      # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n","      y_input = batch_target_seq[:,:-1]\n","      y_expected = batch_target_seq[:,1:]\n","\n","      \n","      # Get mask to mask out the next words\n","      sequence_length = y_input.size(1)\n","      tgt_mask = get_tgt_mask(sequence_length).to(device)\n","\n","      # Standard training except we pass in y_input and tgt_mask\n","      pred = model(X, y_input, tgt_mask=tgt_mask)\n","      loss = loss_fn(rearrange(pred, 'b t v -> (b t) v'), rearrange(y_expected, 'b o -> (b o)'))\n","      \n","\n","      opt.zero_grad()\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n","      loss.backward()\n","      opt.step()\n","  \n","      total_loss += loss.detach().item()\n","\n","        \n","    return total_loss/len(train_loader)"]},{"cell_type":"markdown","metadata":{"id":"GG6RYzNzNBsZ"},"source":["# VALIDATION FUNCTION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJJPwq1NYSMk"},"outputs":[],"source":["def validation_loop(model, loader, vocab_dataset, max_length=25, SOS_token=1, EOS_token=2):\n","    \n","    model.eval()\n","    total_bleu = 0.0\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(loader,0):\n","          \n","            batch_img, batch_target_seq = load_validation(batch)\n","            batch_img, batch_target_seq = batch_img.clone().detach().to(device), batch_target_seq.clone().detach().to(device)\n","            X = batch_img\n","\n","            y_input = torch.ones((len(batch_img), 1), dtype=torch.long, device=device)\n","\n","            for _ in range(max_length):\n","              # Get mask to mask out the next words\n","              sequence_length = y_input.size(1)\n","              tgt_mask = torch.zeros(sequence_length, sequence_length).to(device)\n","\n","              # Standard training except we pass in y_input and src_mask\n","              pred = model(X, y_input, tgt_mask)\n","\n","              # --- GREEDY ---\n","              next_item = pred.topk(k=1, dim=2).indices # num with highest probability\n","              # --- --- --- --\n","              \n","              next_item = next_item[:,-1,:]\n","              # next_item = torch.tensor([[next_item]], device=device)\n","\n","              # Concatenate previous input with predicted best word\n","              y_input = torch.cat((y_input, next_item), dim=1)\n","\n","            prediction = y_input\n","\n","            im = 0\n","            preds = EOS_token * torch.ones_like(prediction)\n","            for i in prediction:\n","                arg = torch.where(i == EOS_token)\n","                try:\n","                    preds[im][:arg[0][0]+1] = prediction[im][:arg[0][0]+1]\n","                except: \n","                    preds[im] = prediction[im]\n","                im += 1\n","            \n","            #bleu-score\n","            bleu = cal_bleu_score(preds, batch_target_seq, 4, vocab_dataset)\n","            total_bleu += bleu\n","        \n","    return total_bleu/len(loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Zsb-hpUvKf-"},"outputs":[],"source":["def beam_search_decoder(X, k=3, max_length=25, SOS_token=int(1), EOS_token=int(2)):\n","  sequences = [[list() + [SOS_token], 0.0]]\n","\t# walk over each step in sequence\n","  for _ in range(max_length-1): # iterate over the max length (building the sentence)\n","    all_candidates = list()\n","\t\t# expand each current candidate\n","    # iterate over the number of sequences we have, i.e. k \n","    for i in range(len(sequences)): \n","      seq, score = sequences[i]\n","\n","      if seq[-1] == EOS_token:\n","        candidate = [seq + [EOS_token], score]\n","        all_candidates.append(candidate)\n","        continue\n","\n","      y_input = torch.tensor([seq], dtype=torch.long, device=device) # dim = (1, len(seq))\n","      sequence_length = y_input.size(1)\n","      tgt_mask = torch.zeros(sequence_length, sequence_length).to(device)\n","\n","      pred = model(X, y_input, tgt_mask)\n","\n","      next_item = pred.topk(k=k, dim=2).indices # num with highest probability\n","      scores = pred.topk(k=k, dim=2).values\n","      next_item = next_item[:,-1,:]\n","      scores = torch.nn.Softmax(dim=1)(scores[:,-1,:])\n","      \n","      # just take top k words and add them to all_candidates\n","      for j in range(k):\n","        candidate = [seq + [next_item[0,j]], score - math.log(scores[0,j])]\n","        all_candidates.append(candidate) # we append all candidates for all the current k sequences\n","        \n","    # order all candidates by score\n","    ordered = sorted(all_candidates, key=lambda tup:tup[1])\n","\n","\t\t# select k best\n","    sequences = ordered[:k]\n","    \n","  # Take the sequence with the highest score, i.e. lowest -log score, i.e. first sequence in sequences\n","  return sequences[0][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STyOdf6UvYiP"},"outputs":[],"source":["def beam_validation_loop(model, loader, vocab_dataset, k=3, max_length=25, SOS_token=1, EOS_token=2):\n","    \n","    model.eval()\n","    total_bleu = 0.0\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(loader,0):\n","            batch_img, batch_target_seq = load_validation(batch)\n","            batch_target_seq = batch_target_seq.clone().detach().to(device) # we use the full batch of targets later on in the Bleu score\n","\n","            # The resulting array of predictions for the whole batch is an array of the size (batch size, max_length)\n","            # max_length=25\n","            prediction = torch.zeros(len(batch_target_seq), max_length, dtype=int).to(device)\n","\n","            # start iterating over the batch here:\n","            for b in range(len(batch_target_seq)):\n","              X = batch_img[b][None, :].clone().detach().to(device)\n","              prediction[b] = torch.tensor(beam_search_decoder(X, k))\n","\n","            #bleu-score\n","            bleu = cal_bleu_score(prediction, batch_target_seq, 4, vocab_dataset)\n","            total_bleu += bleu\n","        \n","    return total_bleu/len(loader)"]},{"cell_type":"markdown","metadata":{"id":"rOv4r97tPOE3"},"source":["# CHECK WHICH GPU WE'RE USING"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":328,"status":"ok","timestamp":1649195314346,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-60},"id":"wdXyGQm-PNXX","outputId":"79bfe3d7-a174-4b61-cb11-7a911d51d1bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Apr  5 21:48:32 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P0    37W / 250W |   1107MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"markdown","metadata":{"id":"pPvLRRhoPWyY"},"source":["# CHECK AMOUNT OF RAM WE HAVE"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1649195316989,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-60},"id":"qTnbdXpjPXJw","outputId":"39622eac-6af1-41bb-d328-6bff786db864"},"outputs":[{"name":"stdout","output_type":"stream","text":["Your runtime has 54.8 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"markdown","metadata":{"id":"lYhHAasVTCC7"},"source":["# Saving and loading checkpoints"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQXXuESOTBZ4"},"outputs":[],"source":["#Save the model state as well as the optimizer in case we want to resume training\n","def save_ckp(state, checkpoint_dir):\n","    f_path = checkpoint_dir + '/checkpoint11.pt'\n","    torch.save(state, f_path)\n","    return\n","\n","def load_ckp(checkpoint_fpath, model, optimizer):\n","    checkpoint = torch.load(checkpoint_fpath+ '/checkpoint11.pt')\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    return model, optimizer, checkpoint['epoch']\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yyDkTc3jNDox"},"source":["# EXECUTION FUNCTION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C-xeVLrWNFEd"},"outputs":[],"source":["def fit(model, opt, loss_fn, train_loader, val_loader, epochs,saving_path,resume_training=False):\n","    \"\"\"\n","    Adapted from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n","    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n","    \"\"\"\n","    if resume_training:\n","      checkpoint_path = saving_path\n","      model, opt, already_trained_epochs = load_ckp(checkpoint_path, model, opt)\n","    \n","    # Used for plotting later on\n","    train_loss_list, validation_bleu_list = [], []\n","    \n","    print(\"Training and validating model\")\n","    curr_best_bleu = 0.\n","\n","    for epoch in range(epochs):\n","        \n","        train_loss = train_loop(model, opt, loss_fn, train_loader)\n","        train_loss_list += [train_loss]\n","\n","        ##Takes too long to compute\n","        # train_bleu = beam_validation_loop(model, train_loader_five, dataset_train_five, max_length=25, SOS_token=1, EOS_token=2)\n","        # train_bleu_list += [train_bleu]\n","\n","        #Save the latest version\n","        print(\"Saving model\")\n","        checkpoint = {\n","              'epoch': epoch,\n","              'state_dict': model.state_dict(),\n","              'optimizer': opt.state_dict()\n","          }\n","\n","        validation_bleu = beam_validation_loop(model, val_loader, valset, max_length=25, SOS_token=1, EOS_token=2)\n","        validation_bleu_list += [validation_bleu]\n","\n","        #Save validation BLEU and training loss to csv loss\n","        df = pd.DataFrame(data={\"val_bleu\": validation_bleu_list, \"train_loss\": train_loss_list})\n","        df.to_csv('/content/drive/MyDrive/GroupProjectNLP/trained_models/results11.csv', sep=',',index=False)\n","\n","        #Only save a new best version if the validation bleu has improved\n","        if validation_bleu > curr_best_bleu:\n","          print(f\"Improved on previous val bleu, saving new model.\")\n","          checkpoint = {\n","              'epoch': epoch,\n","              'state_dict': model.state_dict(),\n","              'optimizer': opt.state_dict()\n","          }\n","          save_ckp(checkpoint, saving_path)\n","          curr_best_bleu = validation_bleu\n","        \n","        if (epoch+1) % 1 == 0:\n","          print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n","          print(f'{epoch+1}: TRAIN LOSS = {train_loss} | VAL BLEU = {validation_bleu}')\n","          print()\n","        \n","    return train_loss_list, validation_bleu_list"]},{"cell_type":"markdown","metadata":{"id":"XMLOdsDoNa7n"},"source":["# SETTING THE MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4U3br67NerC"},"outputs":[],"source":["vocab_size = len(dataset.vocab.stoi)\n","d_model = 50\n","img_feature_size = 1792\n","output_size = 50\n","dim_feedforward=512\n","\n","saving_path = '/content/drive/MyDrive/NLP/trained_models'\n","resume_training = False\n","\n","model = Transformer(\n","    d_model=d_model, nhead=5, num_encoder_layers=3, num_decoder_layers=3, \n","    pos_dropout=0.1, trans_dropout=0.1, dim_feedforward=dim_feedforward,\n","    img_features_size=img_feature_size, output_size=output_size,\n","    vocab_size = vocab_size).to(device)\n","\n","opt = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","loss_fn = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<pad>\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jilv8AkYiARP"},"outputs":[],"source":["train_data_loader = train_loader\n","val_data_loader = val_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0Qzkw_iojQT"},"outputs":[],"source":["train_loss_list, validation_bleu_list = fit(model, opt, loss_fn, train_data_loader, \n","                                                            val_data_loader, epochs=500, saving_path=saving_path,resume_training=resume_training)"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"train_without_detector.ipynb","provenance":[],"authorship_tag":"ABX9TyMpeXgPvWTftUE5a4igjmot"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}