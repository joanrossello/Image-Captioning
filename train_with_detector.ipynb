{"cells":[{"cell_type":"markdown","source":["# Pip Installs"],"metadata":{"id":"nq3eFf9QfNW7"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24989,"status":"ok","timestamp":1649283962234,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-60},"id":"BZGbcEOhVidt","outputId":"e7e4441c-b4a8-4b4c-da7f-55c754ffc3b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting efficientnet_pytorch\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.10.0.2)\n","Building wheels for collected packages: efficientnet-pytorch\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=5a8c9a8ae9d2f44511b197204fd4f2289b71fdc8694add5f9d369b0839d8dfea\n","  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n","Successfully built efficientnet-pytorch\n","Installing collected packages: efficientnet-pytorch\n","Successfully installed efficientnet-pytorch-0.7.1\n","Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n"]}],"source":["!pip install efficientnet_pytorch\n","!pip install spacy\n","from efficientnet_pytorch import EfficientNet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70569,"status":"ok","timestamp":1649284032797,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-60},"id":"kw0tko1Hr2gt","outputId":"c203e66b-f763-4998-9435-4d5d065815c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting bcolz\n","  Downloading bcolz-1.2.1.tar.gz (1.5 MB)\n","\u001b[?25l\r\u001b[K     |▎                               | 10 kB 25.0 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |▊                               | 30 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 92 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 133 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 184 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 225 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 276 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 317 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 327 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 337 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 348 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 358 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 368 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 378 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 389 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 399 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 419 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 430 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 440 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 450 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 460 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 471 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 624 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 645 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 655 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 665 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 675 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 686 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 696 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 706 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 716 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 737 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 747 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 757 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 768 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 778 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 788 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 798 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 808 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 829 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 839 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 849 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 860 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 870 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 880 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 890 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 901 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 921 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 931 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 942 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 952 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 962 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 972 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 983 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 993 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.0 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.0 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.0 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.1 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.1 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.1 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.2 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.2 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.2 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.2 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.2 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.2 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.2 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.3 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.3 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.4 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.4 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5 MB 9.8 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from bcolz) (1.21.5)\n","Building wheels for collected packages: bcolz\n","  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bcolz: filename=bcolz-1.2.1-cp37-cp37m-linux_x86_64.whl size=2649972 sha256=243b3e339689fd371f80329c808df6f65233b623a10bc4988847c11c11bde81f\n","  Stored in directory: /root/.cache/pip/wheels/2c/35/ca/9d914de345914e2446ea285170329f771b8abba2a00f7650bd\n","Successfully built bcolz\n","Installing collected packages: bcolz\n","Successfully installed bcolz-1.2.1\n"]}],"source":["!pip install bcolz\n","import bcolz"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4055,"status":"ok","timestamp":1649284036806,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-60},"id":"NFtmv0efrz2s","outputId":"59f7a726-a818-438c-fd12-1aa496be490b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting einops\n","  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n","Installing collected packages: einops\n","Successfully installed einops-0.4.1\n"]}],"source":["!pip install einops"]},{"cell_type":"markdown","metadata":{"id":"jjjO8X6_Xnaz"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4s_TmhWWOlB"},"outputs":[],"source":["\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch.backends.cudnn as cudnn\n","import numpy as np\n","import torchvision\n","import pickle\n","from torchvision.io import read_image\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import shutil\n","import os\n","import copy\n","import torchvision.transforms as T\n","from torchsummary import summary\n","from torchvision.models import efficientnet_b4,efficientnet_b0\n","import random\n","import pandas as pd\n","import json\n","from efficientnet_pytorch import EfficientNet\n","import spacy\n","from PIL import Image\n","from torch.utils.data import DataLoader, Dataset\n","from spacy.attrs import ORTH\n","import cv2 as cv \n","from google.colab.patches import cv2_imshow # for image display\n","from einops import rearrange\n","import math\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23423,"status":"ok","timestamp":1649284062463,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-60},"id":"kVOMi8PkaLHT","outputId":"0fe48e76-a99d-4b5c-8d25-35b2f6bbec43"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"markdown","metadata":{"id":"lS6ulGHPuVLz"},"source":["# Flickr Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tNJtw6tpxlnI"},"outputs":[],"source":["!unzip '/content/drive/MyDrive/GroupProjectNLP/flickr8k.zip'"]},{"cell_type":"markdown","metadata":{"id":"GT6dKoYqxUUD"},"source":["## Preprocess the images, captions and create a data generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GnIvDj94uXQ9"},"outputs":[],"source":["#Data loader for training: will return only one sentence per image\n","class FlickrDataset(Dataset):\n","    \n","    def __init__(self, root, ann_file, img_transform=None, txt_transform=None):\n","        df = pd.read_csv(ann_file)\n","        self.root = root\n","        self.img_transform = img_transform\n","        self.txt_transform = txt_transform\n","        self.img_ids = df['image']\n","        self.captions = df['caption']\n","        self.vocab = Vocabulary()\n","        self.vocab.build_vocab(self.captions.tolist())\n","    \n","    def __len__(self):\n","        return len(self.img_ids)\n","\n","    def __getitem__(self, idx):\n","        img = Image.open(os.path.join(self.root, self.img_ids[idx]))\n","        img = self.img_transform(img) if self.img_transform is not None else img\n","        caption = self.captions[idx]\n","        caption = self.txt_transform(caption) if self.txt_transform is not None else caption\n","        caption = self.vocab.numericalize('<sos> ' + caption + ' <eos>')\n","        return img, torch.Tensor(caption)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vK1N1lJBww48"},"outputs":[],"source":["#Data loader for validation/testing: will return 5 sentences per image\n","class FlickrDatasetVal(Dataset):\n","    \n","    def __init__(self, root, ann_file, img_transform=None, txt_transform=None):\n","        df = pd.read_csv(ann_file)\n","        self.root = root\n","        self.img_transform = img_transform\n","        self.txt_transform = txt_transform\n","        self.img_ids = df['image']\n","        self.captions = df['captions']\n","        self.vocab = Vocabulary()\n","        self.vocab.build_vocab(self.captions.tolist())\n","    \n","    def __len__(self):\n","        return len(self.img_ids)\n","\n","    def collate_captions(self, captions):\n","        lengths = [len(cap) for cap in captions]\n","        targets = torch.zeros(len(captions), max(lengths)).long()\n","        for i, cap in enumerate([torch.tensor(caption) for caption in captions]):\n","            end = lengths[i]\n","            targets[i, :end] = cap[:end]\n","        return targets  \n","\n","    def __getitem__(self, idx):\n","        img = Image.open(os.path.join(self.root, self.img_ids[idx]))\n","        img = self.img_transform(img) if self.img_transform is not None else img\n","        captions = self.captions[idx].split('|')\n","        captions = ['<sos> ' + caption + ' <eos>' for caption in captions]\n","        captions = self.txt_transform(captions) if self.txt_transform is not None else captions\n","        captions = [self.vocab.numericalize(caption) for caption in captions]        \n","        return img, self.collate_captions(captions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0hdxJwt1I4t"},"outputs":[],"source":["#Preprocessing for both training and validation data\n","def collate_fn(data):\n","    # Sort a data list by caption length (descending order).\n","    data.sort(key=lambda x: len(x[1]), reverse=True)\n","    images, captions = zip(*data)\n","\n","    # Merge images (from tuple of 3D tensor to 4D tensor).\n","    images = torch.stack(images, 0)\n","\n","    # Merge captions (from tuple of 1D tensor to 2D tensor).\n","    lengths = [len(cap) for cap in captions]\n","    targets = torch.zeros(len(captions), max(lengths)).long()\n","    for i, cap in enumerate(captions):\n","        end = lengths[i]\n","        targets[i, :end] = cap[:end]        \n","    return images, targets, lengths\n","\n","\n","def collate_fn_val(data):\n","    images, captions = zip(*data)\n","    images = torch.stack(images, 0)\n","    max_length = max([len(cap[0]) for cap in captions])\n","    targets = torch.zeros(len(captions), 5, max_length).long()\n","    for i, caps in enumerate(captions):\n","        end = caps.shape[1]\n","        targets[i, :, :end] = caps[:end]\n","    return images, targets, max_length\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77P9pCKXvso-"},"outputs":[],"source":["#Building the vocabulary for a given Flickr dataset\n","spacy_eng = spacy.load(\"en_core_web_sm\")\n","spacy_eng.tokenizer.add_special_case('<sos>', [{ORTH: \"<sos>\"}])\n","spacy_eng.tokenizer.add_special_case('<eos>', [{ORTH: \"<eos>\"}])\n","spacy_eng.tokenizer.add_special_case('<pad>', [{ORTH: \"<pad>\"}])\n","spacy_eng.tokenizer.add_special_case('<unk>', [{ORTH: \"<unk>\"}])\n","\n","class Vocabulary:\n","\n","    def __init__(self):\n","        self.itos = {0:\"<pad>\",1:\"<sos>\",2:\"<eos>\",3:\"<unk>\"}\n","        self.stoi = {v:k for k,v in self.itos.items()}\n","        \n","    def __len__(self): return len(self.itos)\n","    \n","    @staticmethod\n","    def tokenize(text):\n","        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n","    \n","    def build_vocab(self, sentence_list):\n","        idx = 4\n","        # add words from all sentences to vocab\n","        for sentence in sentence_list:\n","            for word in self.tokenize(sentence):\n","                if word not in self.stoi:\n","                    self.stoi[word] = idx\n","                    self.itos[idx] = word\n","                    idx += 1\n","    \n","    def numericalize(self,text):\n","        tokenized_text = self.tokenize(text)\n","        return [self.stoi[token] if token in self.stoi else self.stoi[\"<unk>\"] for token in tokenized_text]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6271,"status":"ok","timestamp":1649284116904,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-60},"id":"NmVXsvJZx-wf","outputId":"e534462e-299f-41c5-bec7-e67703ed0395"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}],"source":["# Create train, validation and test data loaders for Flickr8k\n","transform = transforms.Compose([\n","    transforms.Resize([224, 224]),\n","    transforms.ToTensor()\n","])\n","\n","# Training dataset & loader: will only give 1 target sentence per image\n","dataset = FlickrDataset(\n","    root='/content/Images',\n","    ann_file='/content/drive/MyDrive/GroupProjectNLP/flickr8k_train.csv',\n","    img_transform=transform\n",")\n","\n","train_loader = DataLoader(\n","    dataset=dataset,\n","    batch_size=100,\n","    num_workers=8,\n","    shuffle=True,\n","    pin_memory=True,\n","    collate_fn=collate_fn\n",")\n","\n","#To evaluate\n","dataset_train_five = FlickrDatasetVal(\n","    root='/content/Images',\n","    ann_file='/content/drive/MyDrive/GroupProjectNLP/flickr8k_train_agg.csv',\n","    img_transform=transform\n",")\n","\n","train_loader_five = DataLoader(\n","    dataset=dataset_train_five,\n","    batch_size=100,\n","    num_workers=8,\n","    shuffle=False,\n","    pin_memory=True,\n","    collate_fn=collate_fn_val\n",")\n","\n","# Validation\n","valset = FlickrDatasetVal(\n","    root='/content/Images',\n","    ann_file='/content/drive/MyDrive/GroupProjectNLP/flickr8k_val.csv',\n","    img_transform=transform\n",")\n","\n","val_loader = DataLoader(\n","    dataset=valset,\n","    batch_size=100,\n","    num_workers=8,\n","    shuffle=False,\n","    collate_fn=collate_fn_val\n",")\n","\n","# Testing\n","testset = FlickrDatasetVal(\n","    root='/content/Images',\n","    ann_file='/content/drive/MyDrive/GroupProjectNLP/flickr8k_test.csv',\n","    img_transform=transform\n",")\n","\n","test_loader = DataLoader(\n","    dataset=testset,\n","    batch_size=128,\n","    num_workers=8,\n","    shuffle=False,\n","    collate_fn=collate_fn_val\n",")\n","\n","\n","flickr_classes = list(dataset.vocab.stoi.keys())\n","flickr_classes.append(\"<pad>\")"]},{"cell_type":"markdown","metadata":{"id":"Fo6jqBmqXzvi"},"source":["# YOLOv5 model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":439,"referenced_widgets":["5b1ef56ddd2b489a84be8b07573c0c63","23fb78bf05e94f3a94fe5485725d99db","3a6df7ff148b4a3e9206d4830b23e227","a8c5f40267554da680c8e95923bf3ef8","ac74e5a52370443cb9a2b8e71471331c","329b1137de5e4754a95fff0886e52323","549af0bf9ab0496a997f27509c14182a","339742605c844dc499255d94c200c12b","c6124bd656514403a21b9f5bb9f1da7a","a71e06b590ee463596dc37a085096d19","73075971a8bb4087936c5a08f915d2f4"]},"executionInfo":{"elapsed":17085,"status":"ok","timestamp":1649284173016,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-60},"id":"e-9HMVylYLuM","outputId":"73aab3ff-08a4-47ea-ca94-64c5b5538625"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n","Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","\u001b[31m\u001b[1mrequirements:\u001b[0m PyYAML>=5.3.1 not found and is required by YOLOv5, attempting auto-update...\n","Collecting PyYAML>=5.3.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","Installing collected packages: PyYAML\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-6.0\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","YOLOv5 🚀 2022-4-6 torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n","\n"]},{"name":"stdout","output_type":"stream","text":["Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b1ef56ddd2b489a84be8b07573c0c63","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/14.1M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["Fusing layers... \n","YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n","Adding AutoShape... \n"]}],"source":["# Download YOLOv5 model\n","yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True, force_reload=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1649284173016,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-60},"id":"7XMwysKZsciF","outputId":"47290511-99d5-4d52-c07e-bbaff5d1199c"},"outputs":[{"name":"stdout","output_type":"stream","text":["81\n","['scissors', 'teddy bear', 'hair drier', 'toothbrush', '<pad>']\n"]}],"source":["# Yolo classes\n","yolo_classes = yolo_model.names.copy()\n","yolo_classes.append(\"<pad>\")\n","print(len(yolo_classes))\n","print(yolo_classes[-5:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3mEZsoiWOf1"},"outputs":[],"source":["# Set model parameters (optional)\n","yolo_model.conf = 0.35 # confidence threshold (0-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QZZ2gBqWOdm"},"outputs":[],"source":["def transform_yolo(imgs_list):\n","    '''\n","    Function that prepares the data loader images to input in YOLOv5 object detector.\n","    Input: list of images, each image is (n,m,3) pytorch tensor representing normalised [0,1] RGB values for image of dimensions (n,m)\n","    Output: list of images, each image is (n,m,3) numpy array representing un-normalised [0,255] RGB values for image of dimension (n,m)\n","    '''\n","    imgs_list_out = []\n","\n","    for i in range(len(imgs_list)):\n","        imgs_list_out.append(np.array(imgs_list[i]) * 255)\n","\n","    return imgs_list_out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-aDQPJfHWOap"},"outputs":[],"source":["def yolov5_detector(model, imgs_list):\n","    '''\n","    Use YOLOv5 to detect objects in an image.\n","    Input:\n","    model = YOLOv5 model loaded from torch hub\n","    imgs_list = list of images (n,m,3) numpy array with RGB values [0,255]. The images do not necessarily need to be of the same (n,m) size\n","\n","    Outputs:\n","    objects = list of lists of strings of d identified objects (with repetitions) for each image\n","    boxes = list of (d,4) numpy arrays with the bounding boxes coordinates for each object in every image\n","    '''\n","    with torch.no_grad():\n","        results = model(imgs_list)\n","\n","    # Uncomment the line below if you want to print a summary for each image:\n","    # results.print()\n","\n","    data_frames_list = results.pandas().xyxy  # img1 predictions (pandas)\n","\n","    objects = []\n","    boxes = []\n","\n","    for i in range(len(imgs_list)):\n","        objects.append(list(data_frames_list[i]['name']))\n","        boxes.append(results.xyxy[i][:,:4])\n","\n","    return objects, boxes, results"]},{"cell_type":"markdown","metadata":{"id":"duEP1CfFrQ6p"},"source":["# Feature extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["c3feaed11dfc45bb87552bc7ea585b2d","1afeb776de9247aba88e7c30f079e19d","d716e7c83405434e9e2754e0de3e5be7","040639c1b6ed4c689eab5382f620cf4b","cd411a76016a4bf2bbb254b901e3edf6","418801be4de346a99ef43e42620d9fa5","938d5763685646ad8e6911b3c0a5c6ef","3387b8299a1e41f087ca94ef2566dc3e","0ace53f2def84982976875a2072b912f","5c27dad442ba4c768c0926dff23f6b5a","3dfb71ebf93f404f8a34f1d9ff4ae714"]},"executionInfo":{"elapsed":3943,"status":"ok","timestamp":1649284176955,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-60},"id":"_Fz7-jd-rS5b","outputId":"783c7ff1-5d0e-42a6-f4d8-defbc7851617"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b4-6ed6700e.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c3feaed11dfc45bb87552bc7ea585b2d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/74.4M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"]},{"data":{"text/plain":["EfficientNet(\n","  (_conv_stem): Conv2dStaticSamePadding(\n","    3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False\n","    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n","  )\n","  (_bn0): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","  (_blocks): ModuleList(\n","    (0): MBConvBlock(\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        48, 48, kernel_size=(3, 3), stride=[1, 1], groups=48, bias=False\n","        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        48, 12, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        12, 48, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (1): MBConvBlock(\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        24, 24, kernel_size=(3, 3), stride=(1, 1), groups=24, bias=False\n","        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        24, 6, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        6, 24, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (2): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        144, 144, kernel_size=(3, 3), stride=[2, 2], groups=144, bias=False\n","        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        144, 6, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        6, 144, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (3): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n","        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        192, 8, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        8, 192, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (4): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n","        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        192, 8, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        8, 192, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (5): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n","        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        192, 8, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        8, 192, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (6): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        192, 192, kernel_size=(5, 5), stride=[2, 2], groups=192, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        192, 8, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        8, 192, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (7): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        336, 14, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        14, 336, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (8): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        336, 14, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        14, 336, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (9): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        336, 14, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        14, 336, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (10): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        336, 336, kernel_size=(3, 3), stride=[2, 2], groups=336, bias=False\n","        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        336, 14, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        14, 336, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (11): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n","        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        672, 28, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        28, 672, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (12): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n","        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        672, 28, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        28, 672, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (13): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n","        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        672, 28, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        28, 672, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (14): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n","        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        672, 28, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        28, 672, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (15): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n","        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        672, 28, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        28, 672, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (16): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        672, 672, kernel_size=(5, 5), stride=[1, 1], groups=672, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        672, 28, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        28, 672, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (17): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        960, 40, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        40, 960, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (18): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        960, 40, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        40, 960, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (19): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        960, 40, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        40, 960, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (20): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        960, 40, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        40, 960, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (21): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        960, 40, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        40, 960, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (22): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        960, 960, kernel_size=(5, 5), stride=[2, 2], groups=960, bias=False\n","        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        960, 40, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        40, 960, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (23): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (24): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (25): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (26): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (27): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (28): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (29): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n","        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (30): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        1632, 1632, kernel_size=(3, 3), stride=[1, 1], groups=1632, bias=False\n","        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (31): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        2688, 2688, kernel_size=(3, 3), stride=(1, 1), groups=2688, bias=False\n","        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n","      )\n","      (_bn1): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        2688, 112, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        112, 2688, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","  )\n","  (_conv_head): Conv2dStaticSamePadding(\n","    448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False\n","    (static_padding): Identity()\n","  )\n","  (_bn1): BatchNorm2d(1792, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n","  (_dropout): Dropout(p=0.4, inplace=False)\n","  (_fc): Linear(in_features=1792, out_features=1000, bias=True)\n","  (_swish): MemoryEfficientSwish()\n",")"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["#Load model\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model_name = 'efficientnet-b4'\n","classifier_model = EfficientNet.from_pretrained(model_name).to(device)\n","image_size = EfficientNet.get_image_size(model_name)\n","classifier_model.eval()"]},{"cell_type":"markdown","metadata":{"id":"WD4JCHenLUs1"},"source":["# Image Pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-q2wPfjyrVQv"},"outputs":[],"source":["#Image preprocessing, adapted to Daniel's loader\n","\n","def preprocessing_with_aug(imgs):\n","  \n","  gamma = np.random.choice([0.8, 1., 1.2])\n","  imgs = T.functional.adjust_gamma(imgs, gamma)\n","  transf_aug = T.Compose([T.RandomHorizontalFlip(p=0.5),\n","                          T.RandomResizedCrop((224,224)),\n","             ])\n","  imgs = transf_aug(imgs)\n","  return T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(imgs)\n","\n","def preprocessing_no_aug(imgs):\n","  return T.Compose([T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])(imgs)\n"]},{"cell_type":"markdown","metadata":{"id":"lj-9TjrVrxMH"},"source":["# Word embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2pq0IfxurzGU"},"outputs":[],"source":["#Load in pickled embeddings\n","glove_path = \"/content/drive/MyDrive/GroupProjectNLP/\"\n","vectors = bcolz.open(f'{glove_path}/6B.50.dat')[:]\n","words = pickle.load(open(f'{glove_path}/6B.50_words.pkl', 'rb'))\n","word2idx = pickle.load(open(f'{glove_path}/6B.50_idx.pkl', 'rb'))\n","glove = {w: vectors[word2idx[w]] for w in words}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0T4zSrmJMpXG"},"outputs":[],"source":["dataset_embeddings = FlickrDataset(\n","    root='/content/Images',\n","    ann_file='/content/captions.txt',\n","    img_transform=transform\n",")\n","\n","flickr_classes = list(dataset_embeddings.vocab.stoi.keys())\n","flickr_classes.append(\"<pad>\")"]},{"cell_type":"markdown","metadata":{"id":"zISX0HcIu59N"},"source":["## Embedding matrix for Yolo classes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1649284241285,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-60},"id":"jMFhyb2ztVhp","outputId":"390143a8-e5b7-414a-ebef-a35ca3c8a471"},"outputs":[{"data":{"text/plain":["Embedding(81, 50)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# Embedding matrix of Yolo classes\n","embedding_dim = 50\n","matrix_len = len(yolo_classes)\n","weights_matrix = np.zeros((matrix_len, embedding_dim))\n","words_found = 0\n","\n","#Create a matrix of embeddings, not yet directly usable by Pytorch\n","for i, word in enumerate(yolo_classes):\n","    try: \n","        weights_matrix[i] = glove[word]\n","        words_found += 1\n","    except KeyError:\n","        weights_matrix[i] = glove[\"<unk>\"]\n","\n","\n","\n","#Transform the embedding matrix to its Pytorch equivalence\n","weights_matrix = torch.Tensor(weights_matrix)\n","non_trainable = True\n","num_embeddings, embedding_dim = weights_matrix.shape\n","emb_layer_yolo = torch.nn.Embedding(num_embeddings, embedding_dim)\n","emb_layer_yolo.load_state_dict({'weight': weights_matrix})\n","if non_trainable:\n","    emb_layer_yolo.weight.requires_grad = False\n","\n","emb_layer_yolo"]},{"cell_type":"markdown","metadata":{"id":"t-axM-2avAm0"},"source":["## GloVe embedding matrix for Flickr classes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1649284241285,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-60},"id":"QMzF4TbqvDo8","outputId":"69a4df55-96fd-4b6d-b715-ea92e58034e3"},"outputs":[{"data":{"text/plain":["Embedding(8512, 50)"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["# Embedding matrix of flickr classes\n","embedding_dim = 50\n","matrix_len = len(flickr_classes)\n","weights_matrix = np.zeros((matrix_len, embedding_dim))\n","words_found = 0\n","\n","#Create a matrix of embeddings, not yet directly usable by Pytorch\n","for i, word in enumerate(flickr_classes):\n","    try: \n","        weights_matrix[i] = glove[word]\n","        words_found += 1\n","    except KeyError:\n","        weights_matrix[i] = glove[\"<unk>\"]\n","\n","\n","#Transform the embedding matrix to its Pytorch equivalence\n","weights_matrix = torch.Tensor(weights_matrix)\n","non_trainable = True\n","num_embeddings, embedding_dim = weights_matrix.shape\n","emb_layer_flickr = torch.nn.Embedding(num_embeddings, embedding_dim)\n","emb_layer_flickr.load_state_dict({'weight': weights_matrix})\n","if non_trainable:\n","    emb_layer_flickr.weight.requires_grad = False"]},{"cell_type":"markdown","metadata":{"id":"VN4JK4a1LyxA"},"source":["# Function that takes batches and outputs the image features, the object detector vectors, and the target caption indexes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QHOUFNOVWoVI"},"outputs":[],"source":["#For training we do data augmentation\n","def load_training(data):\n","    (imgs, targets, lengths)=data\n","\n","    with torch.no_grad():\n","      \n","      # ------------- FEATURES ----------------------\n","      # ---------------------------------------------\n","      features = classifier_model.extract_features(preprocessing_with_aug(imgs).to(device)).flatten(start_dim=-2).permute(0,2,1)\n","      # ---------------------------------------------\n","\n","      # ------------- OBJECTS -----------------------\n","      # ---------------------------------------------\n","      # Transform images to use them with YOLOv5\n","      imgs = transform_yolo(imgs)\n","\n","      # Apply YOLOv5 to get objects and bounding boxes\n","      objects, boxes, results = yolov5_detector(yolo_model, imgs)\n","\n","      max_objects = len(max(objects, key=len))\n","      for idx,objects_list in enumerate(objects):\n","        pad_amount = (max_objects-len(objects_list))\n","        objects_list.extend([\"<pad>\"]*pad_amount)\n","        boxes[idx] = torch.cat((boxes[idx]/255, torch.zeros((pad_amount,4)).to(device)),0)\n","\n","      indices_objects = torch.tensor([[yolo_classes.index(object_img) for object_img in objects_image] for objects_image in objects])\n","      embedded_objects = emb_layer_yolo(indices_objects)\n","      embedded_objects = torch.cat((embedded_objects.to(device), torch.stack(boxes).to(device)),axis=2)\n","      # ---------------------------------------------\n","\n","      return(features, embedded_objects, targets)\n","\n","\n","def load_validation(data):\n","    (imgs, targets, lengths)=data\n","\n","    with torch.no_grad():    \n","\n","      # ------------- FEATURES ----------------------\n","      # ---------------------------------------------  \n","      features = classifier_model.extract_features(preprocessing_no_aug(imgs).to(device)).flatten(start_dim=-2).permute(0,2,1)\n","      # ---------------------------------------------\n","\n","      # ------------- OBJECTS -----------------------\n","      # ---------------------------------------------\n","      # Transform images to use them with YOLOv5\n","      imgs = transform_yolo(imgs)\n","\n","      # Apply YOLOv5 to get objects and bounding boxes\n","      objects, boxes, results = yolov5_detector(yolo_model, imgs)\n","\n","      max_objects = len(max(objects, key=len))\n","      for idx,objects_list in enumerate(objects):\n","        pad_amount = (max_objects-len(objects_list))\n","        objects_list.extend([\"<pad>\"]*pad_amount)\n","        boxes[idx] = torch.cat((boxes[idx]/255, torch.zeros((pad_amount,4)).to(device)),0)\n","\n","      indices_objects = torch.tensor([[yolo_classes.index(object_img) for object_img in objects_image] for objects_image in objects])\n","      embedded_objects = emb_layer_yolo(indices_objects)\n","      embedded_objects = torch.cat((embedded_objects.to(device), torch.stack(boxes).to(device)),axis=2)\n","      # ---------------------------------------------\n","\n","      return(features, embedded_objects, targets)"]},{"cell_type":"markdown","metadata":{"id":"sSImpeB-MCVt"},"source":["# Positional encoding for the transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_OdggMj0MDrf"},"outputs":[],"source":["# Source: https://pytorch.org/tutorials/beginner/transformer_tutorial\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=100):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)"]},{"cell_type":"markdown","metadata":{"id":"Wcl2SWo-MKKi"},"source":["# Some other needed functions for the transformer (input layer, target mask, etc.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NGrBklhMXAM"},"outputs":[],"source":["class InputLayer(nn.Module):\n","  \"\"\"\n","  Reshaping the image features and appeneding text.\n","  Takes batch of transformed images in the form BxCxWxH and\n","  a batch of words in the form BxNum_of_words x Word_size.\n","\n","  img_feature_size = WxH\n","\n","  Returns Bx(C+Num_of_words) x Word_size\n","  \"\"\"\n","\n","  def __init__(self, img_feature_size, word_size):\n","    super().__init__()\n","\n","    self.fc = nn.Linear(img_feature_size, word_size)\n","\n","  def forward(self, batch_img_feat, batch_words):\n","\n","    x = torch.flatten(batch_img_feat, 2)\n","   \n","    x = self.fc(x)\n","    \n","    y = batch_words\n","    \n","\n","    return torch.cat([x,y], dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3X1qkd1MMdp7"},"outputs":[],"source":["def get_tgt_mask( size) -> torch.tensor:\n","        # Generates a squeare matrix where the each row allows one word more to be seen\n","        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n","        mask = mask.float()\n","        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n","        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n","        \n","        # EX for size=5:\n","        # [[0., -inf, -inf, -inf, -inf],\n","        #  [0.,   0., -inf, -inf, -inf],\n","        #  [0.,   0.,   0., -inf, -inf],\n","        #  [0.,   0.,   0.,   0., -inf],\n","        #  [0.,   0.,   0.,   0.,   0.]]\n","        \n","        return mask\n","    \n","def create_pad_mask( matrix: torch.tensor, pad_token: int) -> torch.tensor:\n","        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n","        # [False, False, False, True, True, True]\n","        return torch.where(matrix==pad_token, True, False)"]},{"cell_type":"markdown","metadata":{"id":"zUiW_8HaMgtQ"},"source":["# The TRANSFORMER model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QN73B5VjMjWc"},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(self, vocab_size, d_model, nhead,\n","                 dim_feedforward=2048, max_seq_length=96, pos_dropout=0.1, word_size=54, img_features_size=64, trans_dropout=0.1\n","                 , num_encoder_layers=6, num_decoder_layers=6):\n","        \"\"\"\n","        Initializes the model\n","                Parameters:\n","                        vocab_size (int): The amount of tokens in both vocabularies (including start, end, etc tokens)\n","                        d_model (int): Expected number of features in the encoder/decoder inputs, also used in embeddings\n","                        nhead (int): Number of heads in the transformer\n","                        num_encoder_layers (int): Number of sub-encoder layers in the transformer\n","                        num_decoder_layers (int): Number of sub-decoder layers in the transformer\n","                        dim_feedforward (int): Dimension of the feedforward network in the transformer\n","                        max_seq_length (int): Maximum length of each tokenized sentence\n","                        pos_dropout (float): Dropout value in the positional encoding\n","                        trans_dropout (float): Dropout value in the transformer\n","        \"\"\"\n","        super().__init__()\n","        self.d_model = d_model\n","        #self.embed_src = nn.Embedding(vocab_size, d_model)\n","        self.input = InputLayer(img_features_size, word_size)\n","\n","        self.embed_tgt = nn.Embedding(vocab_size, d_model)\n","        # self.embed_tgt = emb_layer_flickr\n","        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)\n","\n","        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)\n","        self.fc = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, src_full, tgt, \n","                tgt_mask):\n","        \n","        # merge image and word data\n","        src_img, src_words = src_full\n","        src = self.input(src_img, src_words)\n","        \n","        # change to seq_len x batch x 54\n","        src = torch.permute(src, (1,0,2))\n","\n","        tgt_key_padding_mask = create_pad_mask(tgt,0)\n","        #print(tgt_key_padding_mask, tgt)\n","        tgt = torch.permute(tgt, (1,0))\n","  \n","\n","        # Embed the targets, scale by sqrt(d_model), and add the positional encoding\n","        tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))\n","\n","        # Send the batches to the model\n","        \n","        output = self.transformer(src.float(), tgt.float(), tgt_mask=tgt_mask, \n","                                  tgt_key_padding_mask=tgt_key_padding_mask, \n","                                  )\n","\n","        # change back to batch x seq_len x 54\n","        output = torch.permute(output, (1,0,2))\n","        \n","        # Run the output through an fc layer to return values for each token in the vocab\n","        return self.fc(output)"]},{"cell_type":"markdown","metadata":{"id":"-4A8qalTMt-7"},"source":["# Detokenizer and Bleu Score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":393,"status":"ok","timestamp":1649284859536,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-60},"id":"RVaV3-aGMwZI","outputId":"b19c5d08-a215-4d55-b294-2ab9ff04d6a2"},"outputs":[{"data":{"text/plain":["0.8408964276313782"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["from torchtext.data.metrics import bleu_score\n","candidate_corpus = [['My' , 'full', 'pytorch' ,'test'], ['Another', 'Sentence']]\n","references_corpus = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']], [['No', 'Match']]]\n","bleu_score(candidate_corpus, references_corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-cEtxqsM0Sy"},"outputs":[],"source":["def detokenize(batch):\n","  #During training, there is only one sentence for each image in the batch\n","  (dataset.vocab.itos[4])\n","  temp = batch.tolist()\n","  op = lambda x: dataset.vocab.itos[x]\n","\n","  detokenized= []\n","  for i in range(len(temp)):\n","    withpad = list(map(op, temp[i]))\n","    woutpados = list(filter(lambda x : x not in ['<pad>', '<eos>', '<sos>'], withpad))\n","    detokenized.append(woutpados)\n","  \n","  return detokenized"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sp8C4Q9xJ98R"},"outputs":[],"source":["#When we have multiple target sentences\n","def detokenize_multiple(batch, dataset):\n","  # 5 sentences per image in the batch\n","  (dataset.vocab.itos[4])\n","  batch_size, n_sentences, max_seq_length = batch.shape\n","  op = lambda x: dataset.vocab.itos[x]\n","\n","  detokenized_all= []\n","  for i in range(batch_size):\n","    detokenized_single_image = []\n","    for j in range(n_sentences):\n","      withpad = list(map(op, batch[i,j,:].tolist()))\n","      woutpados = list(filter(lambda x : x not in ['<pad>', '<eos>', '<sos>'], withpad))\n","      detokenized_single_image.append(woutpados)\n","    detokenized_all.append(detokenized_single_image)\n","  \n","  return detokenized_all"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4G6ERReEM3a-"},"outputs":[],"source":["#Compute the average BLEU score across a batch\n","def cal_bleu_score(predictions, targets, n=4, dataset=valset):\n","    targets = detokenize_multiple(targets, dataset)\n","    predictions = detokenize(predictions)\n","    score = round(bleu_score(predictions, targets, max_n=n) * 100, 2)\n","    return score"]},{"cell_type":"markdown","metadata":{"id":"wmxqOapMM56b"},"source":["# TRAINING FUNCTION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fiwF5zNbNAxE"},"outputs":[],"source":["def train_loop(model, opt, loss_fn, train_loader):\n","    model.train()\n","    total_loss = 0.0\n","\n","    for i, batch in enumerate(train_loader,0):\n","\n","      batch_img, batch_words, batch_target_seq = load_training(batch) # image, target, length of target\n","      batch_img, batch_words, batch_target_seq = batch_img.clone().detach().to(device), batch_words.clone().detach().to(device), batch_target_seq.clone().detach().to(device)\n","      X = batch_img, batch_words\n","\n","      # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n","      y_input = batch_target_seq[:,:-1]\n","      y_expected = batch_target_seq[:,1:]\n","\n","      \n","      # Get mask to mask out the next words\n","      sequence_length = y_input.size(1)\n","      tgt_mask = get_tgt_mask(sequence_length).to(device)\n","\n","      # Standard training except we pass in y_input and tgt_mask\n","      pred = model(X, y_input, tgt_mask=tgt_mask)\n","      loss = loss_fn(rearrange(pred, 'b t v -> (b t) v'), rearrange(y_expected, 'b o -> (b o)'))\n","      \n","\n","\n","      opt.zero_grad()\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n","      loss.backward()\n","      opt.step()\n","  \n","      total_loss += loss.detach().item()\n","        \n","    return total_loss/len(train_loader)"]},{"cell_type":"markdown","metadata":{"id":"GG6RYzNzNBsZ"},"source":["# VALIDATION FUNCTION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aN7RJ2vwNDBF"},"outputs":[],"source":["def validation_loop(model, loader, vocab_dataset, max_length=25, SOS_token=1, EOS_token=2):\n","    \n","    model.eval()\n","    total_bleu = 0.0\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(loader,0):\n","          \n","            batch_img, batch_words, batch_target_seq = load_validation(batch) # image, target, length of target\n","            batch_img, batch_words, batch_target_seq = batch_img.clone().detach().to(device), batch_words.clone().detach().to(device), batch_target_seq.clone().detach().to(device)\n","            X = batch_img, batch_words\n","\n","            y_input = torch.ones((len(batch_img), 1), dtype=torch.long, device=device)\n","\n","            for _ in range(max_length):\n","              # Get mask to mask out the next words\n","              sequence_length = y_input.size(1)\n","              tgt_mask = torch.zeros(sequence_length, sequence_length).to(device)\n","\n","              # Standard training except we pass in y_input and src_mask\n","              pred = model(X, y_input, tgt_mask)\n","\n","              # --- GREEDY ---\n","              next_item = pred.topk(k=1, dim=2).indices # num with highest probability\n","              # --- --- --- --\n","              next_item = next_item[:,-1,:] # Get rid of the dummy dimension\n","\n","              # Concatenate previous input with predicted best word\n","              y_input = torch.cat((y_input, next_item), dim=1)\n","\n","            prediction = y_input\n","\n","            im = 0\n","            preds = EOS_token * torch.ones_like(prediction)\n","            for i in prediction:\n","                arg = torch.where(i == EOS_token)\n","                try:\n","                    preds[im][:arg[0][0]+1] = prediction[im][:arg[0][0]+1]\n","                except: \n","                    preds[im] = prediction[im]\n","                im += 1\n","            \n","            #bleu-score\n","            bleu = cal_bleu_score(preds, batch_target_seq, 4, vocab_dataset)\n","            total_bleu += bleu\n","        \n","    return total_bleu/len(loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PWTWqCnyO22O"},"outputs":[],"source":["def beam_search_decoder(X, k=3, max_length=25, SOS_token=int(1), EOS_token=int(2)):\n","  sequences = [[list() + [SOS_token], 0.0]]\n","\t# walk over each step in sequence\n","  for _ in range(max_length-1): # iterate over the max length (building the sentence)\n","    all_candidates = list()\n","\t\t# expand each current candidate\n","    # iterate over the number of sequences we have, i.e. k \n","    for i in range(len(sequences)): \n","      seq, score = sequences[i]\n","\n","      if seq[-1] == EOS_token:\n","        candidate = [seq + [EOS_token], score]\n","        all_candidates.append(candidate)\n","        continue\n","\n","      y_input = torch.tensor([seq], dtype=torch.long, device=device) # dim = (1, len(seq))\n","      sequence_length = y_input.size(1)\n","      tgt_mask = torch.zeros(sequence_length, sequence_length).to(device)\n","\n","      pred = model(X, y_input, tgt_mask)\n","\n","      next_item = pred.topk(k=k, dim=2).indices # num with highest probability\n","      scores = pred.topk(k=k, dim=2).values\n","      next_item = next_item[:,-1,:]\n","      scores = torch.nn.Softmax(dim=1)(scores[:,-1,:])\n","      \n","      # just take top k words and add them to all_candidates\n","      for j in range(k):\n","        candidate = [seq + [next_item[0,j]], score - math.log(scores[0,j])]\n","        all_candidates.append(candidate) # we append all candidates for all the current k sequences\n","        \n","    # order all candidates by score\n","    ordered = sorted(all_candidates, key=lambda tup:tup[1])\n","\n","\t\t# select k best\n","    sequences = ordered[:k]\n","    \n","  # Take the sequence with the highest score, i.e. lowest -log score, i.e. first sequence in sequences\n","  return sequences[0][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BcKezl0RO32O"},"outputs":[],"source":["def beam_validation_loop(model, loader, vocab_dataset, k=3, max_length=25, SOS_token=1, EOS_token=2):\n","    \n","    model.eval()\n","    total_bleu = 0.0\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(loader,0):\n","            batch_img, batch_words, batch_target_seq = load_validation(batch) # image, target, length of target\n","            batch_img, batch_words, batch_target_seq = batch_img.clone().detach().to(device), batch_words.clone().detach().to(device), batch_target_seq.clone().detach().to(device)\n","            \n","            # The resulting array of predictions for the whole batch is an array of the size (batch size, max_length)\n","            # max_length=25\n","            prediction = torch.zeros(len(batch_target_seq), max_length, dtype=int).to(device)\n","\n","            # start iterating over the batch here:\n","            for b in range(len(batch_target_seq)):\n","              X = batch_img[b][None, :].clone().detach().to(device), batch_words[b][None, :].clone().detach().to(device)\n","              prediction[b] = torch.tensor(beam_search_decoder(X, k))\n","\n","            #bleu-score\n","            bleu = cal_bleu_score(prediction, batch_target_seq, 4, vocab_dataset)\n","            total_bleu += bleu\n","        \n","    return total_bleu/len(loader)"]},{"cell_type":"markdown","metadata":{"id":"rOv4r97tPOE3"},"source":["# CHECK WHICH GPU WE'RE USING"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":435,"status":"ok","timestamp":1649284501384,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-60},"id":"wdXyGQm-PNXX","outputId":"5ff8fd13-198a-4f06-bf33-ac7ab96e32c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed Apr  6 22:34:59 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P0    36W / 250W |   1151MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"markdown","metadata":{"id":"pPvLRRhoPWyY"},"source":["# CHECK AMOUNT OF RAM WE HAVE"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1649284503819,"user":{"displayName":"John Russell","userId":"02075401896243230800"},"user_tz":-60},"id":"qTnbdXpjPXJw","outputId":"d5915c62-9464-4131-b72f-fb0b234e6c1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Your runtime has 13.6 gigabytes of available RAM\n","\n","Not using a high-RAM runtime\n"]}],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"markdown","metadata":{"id":"rnEQ46_nPgeu"},"source":["# Save and loading checkpoints"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DvFrknC0Piq5"},"outputs":[],"source":["def save_ckp(state, checkpoint_dir):\n","    f_path = checkpoint_dir + 'checkpoint13.pt'\n","    torch.save(state, f_path)\n","    return\n","\n","def load_ckp(checkpoint_fpath, model, optimizer):\n","    checkpoint = torch.load(checkpoint_fpath+ 'checkpoint13.pt')\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    return model, optimizer, checkpoint['epoch']"]},{"cell_type":"markdown","metadata":{"id":"yyDkTc3jNDox"},"source":["# EXECUTION FUNCTION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C-xeVLrWNFEd"},"outputs":[],"source":["def fit(model, opt, loss_fn, train_loader, val_loader, epochs,saving_path,resume_training=False):\n","    \"\"\"\n","    Adapted from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n","    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n","    \"\"\"\n","    if resume_training:\n","      checkpoint_path = saving_path\n","      model, opt, already_trained_epochs = load_ckp(checkpoint_path, model, opt)\n","    \n","    # Used for plotting later on\n","    train_loss_list, train_bleu_list, validation_bleu_list = [], [], []\n","    \n","    print(\"Training and validating model\")\n","    curr_best_bleu = 0.\n","\n","    for epoch in range(epochs):\n","        \n","        train_loss = train_loop(model, opt, loss_fn, train_loader)\n","        train_loss_list += [train_loss]\n","\n","        print(\"Saving model\")\n","        checkpoint = {\n","              'epoch': epoch,\n","              'state_dict': model.state_dict(),\n","              'optimizer': opt.state_dict()\n","          }\n","        save_ckp(checkpoint, saving_path+'notvalidated_')\n","        # train_bleu = beam_validation_loop(model, train_loader_five, dataset_train_five, max_length=25, SOS_token=1, EOS_token=2)\n","        # train_bleu_list += [train_bleu]\n","\n","        validation_bleu = beam_validation_loop(model, val_loader, valset, max_length=25, SOS_token=1, EOS_token=2)\n","        validation_bleu_list += [validation_bleu]\n","\n","        df = pd.DataFrame(data={\"val_bleu\": validation_bleu_list, \"train_loss\": train_loss_list})\n","        df.to_csv('/content/drive/MyDrive/GroupProjectNLP/trained_models/results13.csv', sep=',',index=False)\n","\n","        if validation_bleu > curr_best_bleu:\n","          print(f\"Improved on previous val bleu, saving new model.\")\n","          checkpoint = {\n","              'epoch': epoch,\n","              'state_dict': model.state_dict(),\n","              'optimizer': opt.state_dict()\n","          }\n","          save_ckp(checkpoint, saving_path)\n","          curr_best_bleu = validation_bleu\n","\n","        \n","        if (epoch+1) % 1 == 0:\n","          print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n","          print(f'{epoch+1}: TRAIN LOSS = {train_loss} | VAL BLEU = {validation_bleu}')\n","          print()\n","        \n","    return train_loss_list, validation_bleu_list"]},{"cell_type":"markdown","metadata":{"id":"XMLOdsDoNa7n"},"source":["# SETTING THE MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4U3br67NerC"},"outputs":[],"source":["vocab_size = len(dataset.vocab.stoi)\n","d_model = 54\n","img_feature_size = 1792\n","output_size = 54\n","dim_feedforward=512\n","\n","saving_path = '/content/drive/MyDrive/GroupProjectNLP/trained_models/'\n","resume_training = False\n","\n","model = Transformer(\n","    d_model=d_model, nhead=2, num_encoder_layers=3, num_decoder_layers=3, \n","    pos_dropout=0.1, trans_dropout=0.1, dim_feedforward=dim_feedforward,\n","    img_features_size=img_feature_size, word_size=output_size,\n","    vocab_size = vocab_size).to(device)\n","\n","opt = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","loss_fn = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<pad>\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0Qzkw_iojQT"},"outputs":[],"source":["train_loss_list, validation_bleu_list = fit(model, opt, loss_fn, train_loader, \n","                                                            val_loader, epochs=500, saving_path=saving_path,resume_training=resume_training)"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"main_with_detector.ipynb","provenance":[],"authorship_tag":"ABX9TyPzVnockYIt8XoNCPRjiORx"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"040639c1b6ed4c689eab5382f620cf4b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c27dad442ba4c768c0926dff23f6b5a","placeholder":"​","style":"IPY_MODEL_3dfb71ebf93f404f8a34f1d9ff4ae714","value":" 74.4M/74.4M [00:02&lt;00:00, 28.6MB/s]"}},"0ace53f2def84982976875a2072b912f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1afeb776de9247aba88e7c30f079e19d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_418801be4de346a99ef43e42620d9fa5","placeholder":"​","style":"IPY_MODEL_938d5763685646ad8e6911b3c0a5c6ef","value":"100%"}},"23fb78bf05e94f3a94fe5485725d99db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_329b1137de5e4754a95fff0886e52323","placeholder":"​","style":"IPY_MODEL_549af0bf9ab0496a997f27509c14182a","value":"100%"}},"329b1137de5e4754a95fff0886e52323":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3387b8299a1e41f087ca94ef2566dc3e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"339742605c844dc499255d94c200c12b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a6df7ff148b4a3e9206d4830b23e227":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_339742605c844dc499255d94c200c12b","max":14808437,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c6124bd656514403a21b9f5bb9f1da7a","value":14808437}},"3dfb71ebf93f404f8a34f1d9ff4ae714":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"418801be4de346a99ef43e42620d9fa5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"549af0bf9ab0496a997f27509c14182a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b1ef56ddd2b489a84be8b07573c0c63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_23fb78bf05e94f3a94fe5485725d99db","IPY_MODEL_3a6df7ff148b4a3e9206d4830b23e227","IPY_MODEL_a8c5f40267554da680c8e95923bf3ef8"],"layout":"IPY_MODEL_ac74e5a52370443cb9a2b8e71471331c"}},"5c27dad442ba4c768c0926dff23f6b5a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73075971a8bb4087936c5a08f915d2f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"938d5763685646ad8e6911b3c0a5c6ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a71e06b590ee463596dc37a085096d19":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8c5f40267554da680c8e95923bf3ef8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a71e06b590ee463596dc37a085096d19","placeholder":"​","style":"IPY_MODEL_73075971a8bb4087936c5a08f915d2f4","value":" 14.1M/14.1M [00:00&lt;00:00, 33.7MB/s]"}},"ac74e5a52370443cb9a2b8e71471331c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3feaed11dfc45bb87552bc7ea585b2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1afeb776de9247aba88e7c30f079e19d","IPY_MODEL_d716e7c83405434e9e2754e0de3e5be7","IPY_MODEL_040639c1b6ed4c689eab5382f620cf4b"],"layout":"IPY_MODEL_cd411a76016a4bf2bbb254b901e3edf6"}},"c6124bd656514403a21b9f5bb9f1da7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd411a76016a4bf2bbb254b901e3edf6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d716e7c83405434e9e2754e0de3e5be7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3387b8299a1e41f087ca94ef2566dc3e","max":77999237,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0ace53f2def84982976875a2072b912f","value":77999237}}}}},"nbformat":4,"nbformat_minor":0}